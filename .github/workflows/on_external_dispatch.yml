name: External Dispatch
on:
  workflow_dispatch:
    inputs:
      duckdb-sha:
        type: string
        description: The DuckDB SHA to build against
        required: true
      force_version:
        type: string
        description: Force version (vX.Y.Z-((rc|post)N))
        required: false
      publish_packages:
        type: boolean
        description: Publish to S3
        required: true
        default: false

jobs:
  externally_triggered_build:
    name: Build and test releases
    uses: ./.github/workflows/pypi_packaging.yml
    with:
      minimal: false
      testsuite: all
      git_ref: ${{ github.ref }}
      duckdb_git_ref: ${{ inputs.duckdb-sha }}
      force_version: ${{ inputs.force_version }}

  publish-s3:
    name: Publish Artifacts to the S3 Staging Bucket
    runs-on: ubuntu-latest
    needs: [ externally_triggered_build ]
    if: ${{ github.repository_owner == 'duckdb' && inputs.publish_packages }}
    steps:
      - name: Fetch artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: '{sdist,wheel}*'
          path: artifacts/
          merge-multiple: true

      - name: Authenticate with AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: 'us-east-2'
          aws-access-key-id: ${{ secrets.S3_DUCKDB_STAGING_ID }}
          aws-secret-access-key: ${{ secrets.S3_DUCKDB_STAGING_KEY }}

      - name: Upload artifacts to S3 bucket
        # semantics: if a version is forced then we upload into a folder by the version name, otherwise we upload
        #   into a folder that is named <run id>-<run-attempt>. Only the latter will be discovered be
        #   upload_to_pypi.yml.
        shell: bash
        run: |
          FOLDER="${{ inputs.force_version != '' && inputs.force_version || format('{0}-{1}', github.run_id, github.run_attempt) }}"
          aws s3 cp artifacts s3://duckdb-staging/${{ github.repository }}/${FOLDER}/ --recursive
